{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from conlleval import eval_f1score\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AutoTokenizer, AutoModel, BertPreTrainedModel, BertModel, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TRAIN = \"data/train/train_dataset.txt\"\n",
    "PATH_VAL = \"data/val/val_dataset.txt\"\n",
    "PATH_AQMAR_TEST = \"data/test/aqmar_test_dataset.txt\"\n",
    "PATH_NEWS_TEST = \"data/test/news_test_dataset.txt\"\n",
    "PATH_TWEETS_TEST = \"data/test/tweets_test_dataset.txt\"\n",
    "FULL_FINETUNE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_id = {\"O\":0, \"B-ORG\":1, \"I-ORG\":2, \"B-PER\":3, \"I-PER\":4, \"B-LOC\":5, \"I-LOC\":6}\n",
    "id_to_label = {value: key for key, value in label_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arabert_tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv01\",do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_label(label):\n",
    "    if \"B-ORG\" in label:\n",
    "        return \"B-ORG\"\n",
    "    elif \"I-ORG\" in label:\n",
    "        return \"I-ORG\"\n",
    "    elif \"B-PER\" in label:\n",
    "        return \"B-PER\"\n",
    "    elif \"I-PER\" in label:\n",
    "        return \"I-PER\"\n",
    "    elif \"B-LOC\" in label:\n",
    "        return \"B-LOC\"\n",
    "    elif \"I-LOC\" in label:\n",
    "        return \"I-LOC\"\n",
    "    elif \"O\" in label:\n",
    "        return \"O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(PATH_DATASET, tokenizer, max_length=512):\n",
    "    data = pd.read_csv(PATH_DATASET, encoding=\"utf-8\", delim_whitespace=True, header=None, skip_blank_lines=False)\n",
    "    Instance = namedtuple(\"Instance\", [\"tokenized_text\", \"input_ids\", \"input_mask\", \"labels\", \"label_ids\"])\n",
    "    dataset = []\n",
    "    text = [\"[CLS]\"]\n",
    "    labels = [\"O\"]\n",
    "    for w, l in zip(data[0], data[1]):\n",
    "        if str(w) == \"nan\" and str(l) == \"nan\":\n",
    "            text.append(\"[SEP]\")\n",
    "            labels.append(\"O\")\n",
    "            \n",
    "            str_text = \" \".join(text)\n",
    "            tokenized_text = arabert_tokenizer.tokenize(str_text)\n",
    "            \n",
    "            cnt = 0 \n",
    "            new_labels = []\n",
    "            label_ids = []\n",
    "            for i in tokenized_text:\n",
    "                if \"##\" in i:\n",
    "                    tok_label = labels[cnt - 1]\n",
    "                    if \"B-\" in tok_label:\n",
    "                        tok_label = tok_label.replace(\"B-\", \"I-\")\n",
    "                        \n",
    "                    tok_label = clean_label(tok_label)\n",
    "                    new_labels.append(tok_label)\n",
    "                    label_ids.append(label_to_id[tok_label])\n",
    "                else:\n",
    "                    new_labels.append(labels[cnt])\n",
    "                    label_ids.append(label_to_id[clean_label(labels[cnt])])\n",
    "                    cnt += 1\n",
    "                                    \n",
    "            input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "                \n",
    "            input_mask = [1] * len(input_ids)\n",
    "            \n",
    "            while len(input_ids) < max_length:\n",
    "                input_ids.append(0)\n",
    "                input_mask.append(0)\n",
    "                label_ids.append(label_to_id[\"O\"])\n",
    "            \n",
    "            dataset.append(Instance(tokenized_text, input_ids,\n",
    "                            input_mask, new_labels, label_ids))\n",
    "\n",
    "            text = [\"[CLS]\"]\n",
    "            labels = [\"O\"]\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        text.append(str(w))\n",
    "        labels.append(str(l))\n",
    "        \n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_tensors(dataset):\n",
    "    tensors_input_ids = []\n",
    "    tensors_input_mask = []\n",
    "    tensors_label_ids = []\n",
    "    for i in dataset:\n",
    "        tensors_input_ids.append(i.input_ids)\n",
    "        tensors_input_mask.append(i.input_mask)\n",
    "        tensors_label_ids.append(i.label_ids)\n",
    "        \n",
    "    return torch.tensor(tensors_input_ids), torch.tensor(tensors_input_mask), torch.tensor(tensors_label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedBertForTokenClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_labels=7):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        outputs =  logits # (logits,) + outputs[2:] add hidden states and attention if they are here\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            # Only keep active parts of the loss\n",
    "            if attention_mask is not None:\n",
    "                active_loss = attention_mask.view(-1) == 1\n",
    "                active_logits = logits.view(-1, self.num_labels)\n",
    "                active_labels = torch.where(\n",
    "                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
    "                )\n",
    "                loss = loss_fct(active_logits, active_labels)\n",
    "            else:\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            outputs =  loss # (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), scores, (hidden_states), (attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, filename, dataset, dataloader):\n",
    "    global id_to_label\n",
    "    model.eval()\n",
    "    f1_score = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        fw =  open(\"{}\".format(filename), \"w\")\n",
    "        cnt = 0\n",
    "        for batch in tqdm(dataloader):\n",
    "            input_ids, input_mask, _ = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            output = model(input_ids=input_ids, attention_mask=input_mask)\n",
    "\n",
    "            length = len(dataset[cnt].tokenized_text)\n",
    "            for w in range(length):\n",
    "                word = dataset[cnt].tokenized_text[w]\n",
    "                true_label = clean_label(dataset[cnt].labels[w])\n",
    "                pred_label = id_to_label[torch.argmax(output.squeeze(0)[w]).item()]\n",
    "                fw.write(\"{} {} {}\\n\".format(word, true_label, pred_label))\n",
    "            fw.write(\"\\n\")\n",
    "            cnt += 1\n",
    "        fw.close()\n",
    "\n",
    "        _, f1_score_arr = eval_f1score(\"{}\".format(filename))\n",
    "        \n",
    "    return f1_score_arr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_dataloader, val_dataloader, accumulation_steps=32, epochs=1, device=\"cpu\"):\n",
    "    model.to(device)\n",
    "    best_f1_score = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        training_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "        cnt_step = 0\n",
    "        for batch in tqdm(train_dataloader):\n",
    "            \n",
    "            input_ids, input_mask, label_ids = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "            \n",
    "            loss = model(input_ids=input_ids, attention_mask=input_mask, labels=label_ids)\n",
    "            training_loss += loss.data.item()\n",
    "            \n",
    "            loss = loss / accumulation_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            if (cnt_step + 1) % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            cnt_step += 1\n",
    "\n",
    "        training_loss /= cnt_step\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_dataloader):\n",
    "                input_ids, input_mask, label_ids = batch\n",
    "                input_ids = input_ids.to(device)\n",
    "                input_mask = input_mask.to(device)\n",
    "                label_ids = label_ids.to(device)\n",
    "\n",
    "                loss = model(input_ids=input_ids, attention_mask=input_mask, labels=label_ids)\n",
    "                val_loss += loss.data.item()\n",
    "\n",
    "            val_loss /= len(val_dataloader)\n",
    "\n",
    "            print(\"epoch {}: training loss {}, val loss {}\".format(epoch, training_loss, val_loss))\n",
    "            \n",
    "        f1_score_arr = evaluate(model, \"val.txt\", dataset_val, val_dataloader)\n",
    "        \n",
    "        if best_f1_score > f1_score_arr[3]:\n",
    "                best_f1_score = f1_score_arr[3]\n",
    "                best_model = model\n",
    "                print(\"We have a better model with an F1 Score: {}\".format(best_f1_score))\n",
    "            \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arabert_model = ModifiedBertForTokenClassification.from_pretrained(\"aubmindlab/bert-base-arabertv01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = preprocess_data(PATH_TRAIN, arabert_tokenizer)\n",
    "dataset_val = preprocess_data(PATH_VAL, arabert_tokenizer)\n",
    "dataset_aqmar_test = preprocess_data(PATH_AQMAR_TEST, arabert_tokenizer)\n",
    "dataset_news_test = preprocess_data(PATH_NEWS_TEST, arabert_tokenizer)\n",
    "dataset_tweets_test = preprocess_data(PATH_TWEETS_TEST, arabert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensors_input_ids, train_tensors_input_mask, train_tensors_label_ids = transform_to_tensors(dataset_train)\n",
    "val_tensors_input_ids, val_tensors_input_mask, val_tensors_label_ids = transform_to_tensors(dataset_val)\n",
    "test_aqmar_tensors_input_ids, test_aqmar_tensors_input_mask, test_aqmar_tensors_label_ids = transform_to_tensors(dataset_aqmar_test)\n",
    "test_news_tensors_input_ids, test_news_tensors_input_mask, test_news_tensors_label_ids = transform_to_tensors(dataset_news_test)\n",
    "test_tweets_tensors_input_ids, test_tweets_tensors_input_mask, test_tweets_tensors_label_ids = transform_to_tensors(dataset_tweets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor_dataset = TensorDataset(train_tensors_input_ids, train_tensors_input_mask, train_tensors_label_ids)\n",
    "val_tensor_dataset = TensorDataset(val_tensors_input_ids, val_tensors_input_mask, val_tensors_label_ids)\n",
    "test_aqmar_tensor_dataset = TensorDataset(test_aqmar_tensors_input_ids, test_aqmar_tensors_input_mask, test_aqmar_tensors_label_ids)\n",
    "test_news_tensor_dataset = TensorDataset(test_news_tensors_input_ids, test_news_tensors_input_mask, test_news_tensors_label_ids)\n",
    "test_tweets_tensor_dataset = TensorDataset(test_tweets_tensors_input_ids, test_tweets_tensors_input_mask, test_tweets_tensors_label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_tensor_dataset, batch_size=1)\n",
    "val_dataloader = DataLoader(val_tensor_dataset, batch_size=1)\n",
    "test_aqmar_dataloader = DataLoader(test_aqmar_tensor_dataset, batch_size=1)\n",
    "test_news_dataloader = DataLoader(test_news_tensor_dataset, batch_size=1)\n",
    "test_tweets_dataloader = DataLoader(test_tweets_tensor_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_grouped_parameters = None\n",
    "param_optimizer = list(arabert_model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "\n",
    "if FULL_FINETUNE:\n",
    "    print('ALL FINETUNE')\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    print('NO ALL FINETUNE')\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': arabert_model.classifier.parameters(),\n",
    "         'weight_decay_rate': 0.01}\n",
    "    ]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = train(arabert_model, optimizer, train_dataloader, val_dataloader, epochs=1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(trained_model, \"val.txt\", dataset_val, val_dataloader)\n",
    "evaluate(trained_model, \"test_aqmar.txt\", dataset_aqmar_test, test_aqmar_dataloader)\n",
    "evaluate(trained_model, \"test_news.txt\", dataset_news_test, test_news_dataloader)\n",
    "evaluate(trained_model, \"test_tweets.txt\", dataset_tweets_test, test_tweets_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
