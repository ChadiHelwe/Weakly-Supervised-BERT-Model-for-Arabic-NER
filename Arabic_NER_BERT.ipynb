{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel, BertPreTrainedModel, BertModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TRAIN = \"data/train/train_dataset.txt\"\n",
    "PATH_VAL = \"data/val/val_dataset.txt\"\n",
    "PATH_AQMAR_TEST = \"data/test/aqmar_test_dataset.txt\"\n",
    "PATH_NEWS_TEST = \"data/test/news_test_dataset.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_id = {\"O\":0, \"B-ORG\":1, \"I-ORG\":2, \"B-PER\":3, \"I-PER\":4, \"B-LOC\":5, \"I-LOC\":6}\n",
    "id_to_label = {value: key for key, value in label_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "arabert_tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv01\",do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(PATH_DATASET, tokenizer, max_length=512):\n",
    "    data = pd.read_csv(PATH_DATASET, delim_whitespace=True, header=None, skip_blank_lines=False)\n",
    "    dataset = []\n",
    "    tweet = [\"[CLS]\"]\n",
    "    labels = [\"O\"]\n",
    "    for w, l in zip(data[0], data[1]):\n",
    "        if str(w) == \"nan\" and str(l) == \"nan\":\n",
    "            tweet.append(\"[SEP]\")\n",
    "            labels.append(\"O\")\n",
    "            \n",
    "            str_tweet = \" \".join(tweet)\n",
    "            tokenized_tweet = arabert_tokenizer.tokenize(str_tweet)\n",
    "            \n",
    "            cnt = 0 \n",
    "            new_labels = []\n",
    "            label_ids = []\n",
    "            for i in tokenized_tweet:\n",
    "                if \"##\" in i:\n",
    "                    tok_label = labels[cnt - 1]\n",
    "                    if \"B-\" in tok_label:\n",
    "                        tok_label = tok_label.replace(\"B-\", \"I-\")\n",
    "                    new_labels.append(tok_label)\n",
    "                    label_ids.append(label_to_id[tok_label])\n",
    "                else:\n",
    "                    new_labels.append(labels[cnt])\n",
    "                    label_ids.append(label_to_id[labels[cnt]])\n",
    "                    cnt += 1\n",
    "                                    \n",
    "            input_ids = tokenizer.convert_tokens_to_ids(tokenized_tweet)\n",
    "                \n",
    "            input_mask = [1] * len(input_ids)\n",
    "            \n",
    "            while len(input_ids) < max_length:\n",
    "                input_ids.append(0)\n",
    "                input_mask.append(0)\n",
    "            \n",
    "            dataset.append([tokenized_tweet, input_ids, input_mask, new_labels, label_ids])\n",
    "\n",
    "            tweet = [\"[CLS]\"]\n",
    "            labels = [\"O\"]\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        tweet.append(str(w))\n",
    "        labels.append(str(l))\n",
    "        \n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedBertForTokenClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_labels=7):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            # Only keep active parts of the loss\n",
    "            if attention_mask is not None:\n",
    "                active_loss = attention_mask.view(-1) == 1\n",
    "                active_logits = logits.view(-1, self.num_labels)\n",
    "                active_labels = torch.where(\n",
    "                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
    "                )\n",
    "                loss = loss_fct(active_logits, active_labels)\n",
    "            else:\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), scores, (hidden_states), (attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "arabert_model = ModifiedBertForTokenClassification.from_pretrained(\"aubmindlab/bert-base-arabertv01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = preprocess_data(PATH_TRAIN, arabert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'ففي', 'مساف', '##ه', '300', '##0', 'متر', 'موانع', 'خالف', 'العداء', 'المغربي', 'عبد', 'الغني', 'آيت', 'باح', '##ما', '##د', 'التوقعات', 'التي', 'كانت', 'ترشحه', 'للفوز', 'باحد', '##ي', 'الميداليات', 'واكتف', '##ي', 'بالمرتب', '##ه', 'الرابع', '##ه', 'في', 'المس', '##ابق', '##ه', 'النهائي', '##ه', 'لهذه', 'المس', '##ا', '##ف', '##ه', 'مسجلا', 'توقيت', '8', '##د', '##و', '##20', '##ث', '##و', '##05', '##ج', '[SEP]']\n",
      "[17028, 3020, 11335, 909, 1128, 808, 3475, 25813, 8167, 33405, 47140, 2811, 19131, 1259, 1561, 4768, 891, 53091, 5878, 10348, 20847, 24464, 6251, 912, 59421, 26498, 912, 48065, 909, 32741, 909, 660, 6081, 14660, 909, 47500, 909, 10929, 6081, 883, 903, 909, 25262, 21440, 18, 891, 910, 4288, 887, 910, 4271, 888, 17030, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.2745,  0.6423, -0.8280,  ..., -0.4021,  0.4501,  0.4468],\n",
       "          [ 0.0801,  0.4459, -0.9089,  ..., -0.4047,  0.1387,  0.3629],\n",
       "          [ 0.1287,  0.4684, -0.9252,  ..., -0.3287,  0.3051,  0.3348],\n",
       "          ...,\n",
       "          [ 0.2671,  0.7023, -0.8001,  ..., -0.3694,  0.4411,  0.4452],\n",
       "          [ 0.2644,  0.6985, -0.8136,  ..., -0.3863,  0.4299,  0.4377],\n",
       "          [ 0.2750,  0.7018, -0.8385,  ..., -0.4575,  0.4643,  0.4280]]],\n",
       "        grad_fn=<AddBackward0>),)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset[0][0])\n",
    "print(dataset[0][1])\n",
    "print(dataset[0][2])\n",
    "print(dataset[0][3])\n",
    "\n",
    "arabert_model(torch.tensor([dataset[0][1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
