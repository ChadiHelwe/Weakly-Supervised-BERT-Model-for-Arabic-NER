{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TRAIN = \"data/train/train_dataset.txt\"\n",
    "PATH_VAL = \"data/val/val_dataset.txt\"\n",
    "PATH_AQMAR_TEST = \"data/test/aqmar_test_dataset.txt\"\n",
    "PATH_NEWS_TEST = \"data/test/news_test_dataset.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arabert_tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv01\",do_lower_case=False)\n",
    "arabert_model = AutoModel.from_pretrained(\"aubmindlab/bert-base-arabertv01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(PATH_DATASET, tokenizer):\n",
    "    data = pd.read_csv(PATH_DATASET, delim_whitespace=True, header=None, skip_blank_lines=False)\n",
    "    dataset = []\n",
    "    tweet = [\"[CLS]\"]\n",
    "    labels = [\"O\"]\n",
    "    for w, l in zip(data[0], data[1]):\n",
    "        if str(w) == \"nan\" and str(l) == \"nan\":\n",
    "            tweet.append(\"[SEP]\")\n",
    "            labels.append(\"O\")\n",
    "            \n",
    "            str_tweet = \" \".join(tweet)\n",
    "            tokenized_tweet = arabert_tokenizer.tokenize(str_tweet)\n",
    "            \n",
    "            cnt = 0 \n",
    "            new_labels = []\n",
    "            for i in tokenized_tweet:\n",
    "                if \"##\" in i:\n",
    "                    tok_label = labels[cnt - 1]\n",
    "                    if \"B-\" in tok_label:\n",
    "                        tok_label = tok_label.replace(\"B-\", \"I-\")\n",
    "                    new_labels.append(tok_label)\n",
    "                else:\n",
    "                    new_labels.append(labels[cnt])\n",
    "                    cnt += 1\n",
    "            dataset.append([tokenized_tweet, new_labels])\n",
    "\n",
    "            tweet = [\"[CLS]\"]\n",
    "            labels = [\"O\"]\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        tweet.append(str(w))\n",
    "        labels.append(str(l))\n",
    "        \n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = preprocess_data(PATH_NEWS_TEST, arabert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] O\n",
      "قالت O\n",
      "السلطات O\n",
      "الالماني O\n",
      "##ه O\n",
      "ان O\n",
      "من O\n",
      "المحتمل O\n",
      "جدا O\n",
      "ان O\n",
      "يكون O\n",
      "اكبر O\n",
      "تف O\n",
      "##ش O\n",
      "لتس O\n",
      "##م O\n",
      "##م O\n",
      "غذاء O\n",
      "##ي O\n",
      "شهدته O\n",
      "المانيا B-LOC\n",
      "والذي O\n",
      "اصاب O\n",
      "اكثر O\n",
      "من O\n",
      "11 O\n",
      "الف O\n",
      "تلميذ O\n",
      "بحالات O\n",
      "اس O\n",
      "##ها O\n",
      "##ل O\n",
      "وق O\n",
      "##ي O\n",
      "##ء O\n",
      "نجم O\n",
      "عن O\n",
      "كم O\n",
      "##ي O\n",
      "##ه O\n",
      "من O\n",
      "الفرا O\n",
      "##وله O\n",
      "المجمد O\n",
      "##ه O\n",
      "وت O\n",
      "##ا O\n",
      "##ثر O\n",
      "بذلك O\n",
      "اطفال O\n",
      "في O\n",
      "نحو O\n",
      "500 O\n",
      "مدرس O\n",
      "##ه O\n",
      "ومركز O\n",
      "لل O\n",
      "##ر O\n",
      "##ع O\n",
      "##ا O\n",
      "##ي O\n",
      "##ه O\n",
      "الاجتماعي O\n",
      "##ه O\n",
      "في O\n",
      "شت O\n",
      "##ي O\n",
      "ان O\n",
      "##حاء O\n",
      "شرق O\n",
      "المانيا B-LOC\n",
      "تلقت O\n",
      "هذا O\n",
      "الطعام O\n",
      "من O\n",
      "مقاول O\n",
      "من O\n",
      "الباطن O\n",
      "لشرك O\n",
      "##ه O\n",
      "سود B-ORG\n",
      "##يكس I-ORG\n",
      "##و I-ORG\n",
      "لتوريد O\n",
      "الاغذي O\n",
      "##ه O\n",
      "وان O\n",
      "ما O\n",
      "لا O\n",
      "يقل O\n",
      "عن O\n",
      "32 O\n",
      "طفلا O\n",
      "تطلب O\n",
      "الامر O\n",
      "علاجهم O\n",
      "في O\n",
      "المستشفي O\n",
      "وقال O\n",
      "معهد B-ORG\n",
      "روبرت I-ORG\n",
      "كوتش I-ORG\n",
      "الذي O\n",
      "يقدم O\n",
      "المش O\n",
      "##ور O\n",
      "##ه O\n",
      "لوز O\n",
      "##اره O\n",
      "الصح O\n",
      "##ه O\n",
      "الالماني O\n",
      "##ه O\n",
      "في O\n",
      "بيان O\n",
      "امس O\n",
      "انه O\n",
      "بشان O\n",
      "الامراض O\n",
      "المعد O\n",
      "##ي O\n",
      "##ه O\n",
      "فقد O\n",
      "وجد O\n",
      "صل O\n",
      "##ه O\n",
      "قوي O\n",
      "##ه O\n",
      "وذات O\n",
      "[SEP] O\n"
     ]
    }
   ],
   "source": [
    "for i, j in zip(dataset[-1][0], dataset[-1][1]):\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
